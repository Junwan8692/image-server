from flask import Flask, request, jsonify, render_template, send_file
import os, json, uuid, requests, re, base64
from io import BytesIO
from PIL import Image
import traceback

# [ì¶”ê°€] Google Drive API ê´€ë ¨ ì„í¬íŠ¸
from googleapiclient.discovery import build
from google.oauth2.service_account import Credentials
from googleapiclient.http import MediaIoBaseUpload

app = Flask(__name__)

JOB_DIR = "jobs"
OUTPUT_DIR = "output"
os.makedirs(JOB_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# âœ… ì´ë¯¸ì§€ ìºì‹œ
IMAGE_CACHE = {}
THUMB_MAX_SIZE = 720  # ê°€ì´ë“œ/í”„ë¦¬ë·°ìš© ì¸ë„¤ì¼ ì„¸ë¡œ ìµœëŒ€ í¬ê¸°

# [ì¶”ê°€] Google Drive ì¸ì¦ ì„¤ì •
SCOPES = ['https://www.googleapis.com/auth/drive.file']
# [ì£¼ì˜] ì´ íŒŒì¼ì´ app.pyì™€ ê°™ì€ ìœ„ì¹˜ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
SERVICE_ACCOUNT_FILE = 'service_account.json'

def get_gdrive_service():
    """Google Drive API ì„œë¹„ìŠ¤ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        creds = Credentials.from_service_account_file(
            SERVICE_ACCOUNT_FILE, scopes=SCOPES)
        service = build('drive', 'v3', credentials=creds)
        return service
    except Exception as e:
        print(f"âŒ Google Drive ì¸ì¦ ì‹¤íŒ¨: {e}")
        print(f"âš ï¸ '{SERVICE_ACCOUNT_FILE}' íŒŒì¼ì´ í•„ìš”í•˜ë©°, Google Drive APIê°€ í™œì„±í™”ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")
        print("âš ï¸ ë˜í•œ, ëŒ€ìƒ ë“œë¼ì´ë¸Œ í´ë”ê°€ ì´ ì„œë¹„ìŠ¤ ê³„ì • ì´ë©”ì¼ê³¼ 'ê³µìœ 'ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")
        return None


def get_image_cached(url):
    """Google Drive ì´ë¯¸ì§€ ìºì‹± (ì›ë³¸ + ì¸ë„¤ì¼)"""
    if url in IMAGE_CACHE:
        return IMAGE_CACHE[url]

    try:
        r = requests.get(url, timeout=10)
        # ğŸ‘ˆ [ì°¸ê³ ] .convert("RGB")ê°€ PNGì˜ íˆ¬ëª…ë„ë¥¼ í°ìƒ‰/ê²€ì€ìƒ‰ ë°°ê²½ìœ¼ë¡œ ìë™ ë³‘í•©í•©ë‹ˆë‹¤.
        img = Image.open(BytesIO(r.content)).convert("RGB")

        # ì¸ë„¤ì¼ ìƒì„±
        preview = img.copy()
        preview.thumbnail((THUMB_MAX_SIZE, THUMB_MAX_SIZE))

        IMAGE_CACHE[url] = (img, preview)
        return (img, preview)
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ìºì‹œ ì‹¤íŒ¨: {url} ({e})")
        return (None, None)


# --------------------------------------------------
# 1ï¸âƒ£ n8n â†’ Flask
# --------------------------------------------------
@app.route("/job", methods=["POST"])
def receive_job():
    data = request.get_json()
    job_id = str(uuid.uuid4())

    # [ìˆ˜ì •] n8nì—ì„œ gdrive_folder_urlì„ ë°›ì•„ ID ì¶”ì¶œ
    gdrive_url = data.get("gdrive_folder_url")
    gdrive_folder_id = None
    if gdrive_url:
        gdrive_folder_id = get_gdrive_folder_id(gdrive_url)
        if gdrive_folder_id:
            data["gdrive_folder_id"] = gdrive_folder_id # job dataì— ID ì €ì¥
            print(f"ğŸ¯ Google Drive ìƒìœ„ í´ë” ID ì €ì¥ë¨: {gdrive_folder_id}")
        else:
            print(f"âš ï¸ Google Drive URLì—ì„œ í´ë” IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {gdrive_url}")
            data["gdrive_folder_id"] = None # IDê°€ ì—†ìŒì„ ëª…ì‹œ
    else:
        print("â„¹ï¸ n8n ìš”ì²­ì— 'gdrive_folder_url'ì´ ì—†ìŠµë‹ˆë‹¤.")
        data["gdrive_folder_id"] = None

    # â­ï¸ [ê³ ë„í™”] n8n ì›¹í›… URL ì €ì¥
    n8n_webhook_url = data.get("n8n_webhook_url")
    if n8n_webhook_url:
        data["n8n_webhook_url"] = n8n_webhook_url
        print(f"ğŸ”” n8n ì™„ë£Œ ì•Œë¦¼ Webhook URL ì €ì¥ë¨: {n8n_webhook_url}")
    else:
        print("â„¹ï¸ n8n ìš”ì²­ì— 'n8n_webhook_url'ì´ ì—†ìŠµë‹ˆë‹¤. ì™„ë£Œ ì•Œë¦¼ì„ ë³´ë‚´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        data["n8n_webhook_url"] = None

    job_path = os.path.join(JOB_DIR, f"{job_id}.json")
    with open(job_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    # [ìˆ˜ì •] IP ì£¼ì†Œë¥¼ ngrok ì£¼ì†Œë¡œ ë³€ê²½
    #gui_url = f"https://1a8ef1b025b2.ngrok-free.app/gui/{job_id}" # ğŸ‘ˆ 5000ë²ˆ í¬íŠ¸ìš© ngrok ì£¼ì†Œ
    gui_url = f"http://172.16.16.109:5000/gui/{job_id}" # ğŸ‘ˆ IP í™•ì¸ í•„ìš”
    return jsonify({"ok": True, "job_id": job_id, "gui_url": gui_url})


# --------------------------------------------------
# 2ï¸âƒ£ GUI í˜ì´ì§€
# --------------------------------------------------
@app.route("/gui/<job_id>")
def gui(job_id):
    job_path = os.path.join(JOB_DIR, f"{job_id}.json")
    if not os.path.exists(job_path):
        return "âŒ Job not found", 404
    with open(job_path, "r", encoding="utf-8") as f:
        job_data = json.load(f)

    return render_template("gui.html", job_id=job_id, job=job_data)


# --------------------------------------------------
# 3ï¸âƒ£ ì´ë¯¸ì§€ í”„ë¡ì‹œ
# --------------------------------------------------
@app.route("/image_proxy")
def image_proxy():
    url = request.args.get("url")
    if not url:
        return "Missing URL", 400

    try:
        _, preview = get_image_cached(url)
        if preview is None:
            return "âŒ Preview fetch failed", 500

        buf = BytesIO()
         # ğŸ‘ˆ [ì°¸ê³ ] í”„ë¡ì‹œ ì´ë¯¸ì§€ëŠ” í•­ìƒ JPEGë¡œ ë³€í™˜ë˜ì–´ ì „ì†¡ë©ë‹ˆë‹¤.
        preview.save(buf, format="JPEG", quality=70)
        buf.seek(0)
        return send_file(buf, mimetype="image/jpeg")
    except Exception as e:
        return f"Proxy Error: {str(e)}", 500


# --------------------------------------------------
# 4ï¸âƒ£ ìœ í‹¸
# --------------------------------------------------
def merge_images_vertically(images):
    widths, heights = zip(*(i.size for i in images))
    total_height = sum(heights)
    merged = Image.new("RGB", (widths[0], total_height))
    y_offset = 0
    for img in images:
        merged.paste(img, (0, y_offset))
        y_offset += img.height
    return merged


# â­ï¸â­ï¸â­ï¸ [ìˆ˜ì •ëœ ë¶€ë¶„] â­ï¸â­ï¸â­ï¸
def get_adjacent_names(base_name):
    """
    íŒŒì¼ ì´ë¦„(ì˜ˆ: "scene_001.png")ì„ ë°›ì•„, ì•ë’¤ ë²ˆí˜¸ì˜ íŒŒì¼ ì´ë¦„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    .jpg, .jpeg, .png, .webp í™•ì¥ìë¥¼ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
    """
    # [ìˆ˜ì •] .jpg, .jpeg, .png, .webp í™•ì¥ìë¥¼ ëª¨ë‘ ì°¾ë„ë¡ ì •ê·œì‹ ë³€ê²½ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
    m = re.search(r"(\d+)\.(jpg|jpeg|png|webp)$", base_name, re.IGNORECASE)
    
    if not m:
        # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ (ì˜ˆ: "title.jpg" ê°™ì´ ìˆ«ì íŒ¨í„´ì´ ì—†ì„ ë•Œ), ì›ë³¸ ì´ë¦„ë§Œ ë°˜í™˜
        return [base_name]

    num_str = m.group(1)    # ìˆ«ì ë¬¸ìì—´ (ì˜ˆ: "011")
    extension = m.group(2)  # í™•ì¥ì (ì˜ˆ: "jpg" ë˜ëŠ” "png")
    
    num = int(num_str)
    digits = len(num_str) # ìë¦¿ìˆ˜ (ì˜ˆ: 3)
    
    # íŒŒì¼ ì´ë¦„ì—ì„œ ìˆ«ìì™€ í™•ì¥ì ë¶€ë¶„ì„ ì œì™¸í•œ ì•ë¶€ë¶„(prefix)ì„ ì°¾ìŠµë‹ˆë‹¤.
    # m.start(1)ì€ ìˆ«ì ë¬¸ìì—´("011")ì´ ì‹œì‘ë˜ëŠ” ì¸ë±ìŠ¤ì…ë‹ˆë‹¤.
    base_prefix = base_name[:m.start(1)] # ì˜ˆ: "ì—¬ê²Œì•„_1í™”_"
    
    # ì›ë³¸ í™•ì¥ìë¥¼ ìœ ì§€í•˜ë©´ì„œ íŒŒì¼ ì´ë¦„ ì¬ì¡°í•©
    prev_name = f"{base_prefix}{str(num - 1).zfill(digits)}.{extension}"
    next_name = f"{base_prefix}{str(num + 1).zfill(digits)}.{extension}"

    return [prev_name, base_name, next_name]

def get_gdrive_folder_id(url):
    """Google Drive í´ë” URL(ì¼ë°˜/ê³µìœ  ë“œë¼ì´ë¸Œ)ì—ì„œ í´ë” IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    match = re.search(r'folders/([a-zA-Z0-9_-]+)', url)
    if match:
        return match.group(1)
    match = re.search(r'id=([a-zA-Z0-9_-]+)', url)
    if match:
        return match.group(1)
    return None


def load_job(job_id):
    """ì§€ì •ëœ job_idì— í•´ë‹¹í•˜ëŠ” job íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not job_id or ".." in job_id or "/" in job_id or "\\" in job_id:
        print(f"âŒ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ì•ˆì „í•˜ì§€ ì•Šì€ job_id ìš”ì²­: {job_id}")
        return None

    job_path = os.path.join(JOB_DIR, f"{job_id}.json")
    if not os.path.exists(job_path):
        print(f"âŒ Job íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {job_path}")
        return None

    try:
        with open(job_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ Job íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {job_path} ({e})")
        return None

def create_gdrive_folder(service, folder_name, parent_folder_id):
    """(ê³µìœ ) ë“œë¼ì´ë¸Œì— ìƒˆ í´ë”ë¥¼ ë§Œë“¤ê³  IDë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        file_metadata = {
            'name': folder_name,
            'mimeType': 'application/vnd.google-apps.folder',
            'parents': [parent_folder_id]
        }
        folder = service.files().create(
            body=file_metadata,
            fields='id',
            supportsAllDrives=True
        ).execute()

        new_folder_id = folder.get('id')
        print(f"âœ… Google Drive í´ë” ìƒì„± ì„±ê³µ: {folder_name} (ID: {new_folder_id})")
        return new_folder_id
    except Exception as e:
        # í´ë”ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ (4xx ì—ëŸ¬)
        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” í´ë”ë¥¼ ì°¾ì•„ì„œ IDë¥¼ ë°˜í™˜í•˜ëŠ” ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ë„ ìˆìŒ
        print(f"âŒ Google Drive í´ë” ìƒì„± ì‹¤íŒ¨ (ì´ë¦„: {folder_name}, ë¶€ëª¨: {parent_folder_id}): {e}")
        # traceback.print_exc() # ìƒì„¸ ì—ëŸ¬ ë¡œê·¸ í•„ìš” ì‹œ ì£¼ì„ í•´ì œ
        return None

# --------------------------------------------------
# 5ï¸âƒ£ Preview (ì¸ë„¤ì¼ ê¸°ì¤€, 3ì—´)
# --------------------------------------------------
@app.route("/preview_crops", methods=["POST"])
def preview_crops():
    from base64 import b64encode

    payload = request.get_json()
    job_id = payload.get("job_id")
    data = payload.get("guides", {})

    if not job_id:
        return jsonify({"ok": False, "message": "Job IDê°€ ì—†ìŠµë‹ˆë‹¤."}), 400

    print(f"ğŸ‘€ Preview Request Received: Job ID {job_id}, {len(data)} items")

    previews = {}

    job_data = load_job(job_id)
    if not job_data:
        return jsonify({"ok": False, "message": f"Job not found (ID: {job_id})."}), 400

    if "scenes" not in job_data:
        print(f"âŒ Job íŒŒì¼ì— 'scenes' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. (ID: {job_id})")
        return jsonify({"ok": False, "message": "Job data is incomplete (missing 'scenes')."}), 400

    job_output_dir = os.path.join(OUTPUT_DIR, job_id)
    os.makedirs(job_output_dir, exist_ok=True)

    image_map = {img["name"]: img["url"] for img in job_data.get("all_images", [])}

    sorted_items = sorted(data.items(), key=lambda x: int(x[0].split("_")[0]))

    for key, crop in sorted_items:
        # ğŸ‘ˆ [ì°¸ê³ ] í”„ë¦¬ë·° íŒŒì¼ì€ í•­ìƒ .jpgë¡œ ì €ì¥ë©ë‹ˆë‹¤.
        img_path = os.path.join(job_output_dir, f"{key}.jpg")

        if not os.path.exists(img_path):
            print(f"âš ï¸ Missing cropped file: {img_path} â€” generating preview...")
            try:
                scene_num, cut_type = key.split("_")

                scene_info = next(
                    (s for s in job_data["scenes"] if str(s["scene_number"]) == scene_num),
                    None,
                )
                if not scene_info:
                    print(f"âš ï¸ ì”¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: ì”¬ {scene_num}")
                    continue

                base_name = (
                    scene_info["primary_cut"]
                    if "primary" in cut_type
                    else scene_info["alternative_cuts"][int(cut_type[-1]) - 1]
                )

                # ğŸ‘ˆ [ìˆ˜ì •] ì´ í•¨ìˆ˜ê°€ ì´ì œ PNGë„ ì˜ ì²˜ë¦¬í•©ë‹ˆë‹¤.
                neighbor_names = get_adjacent_names(base_name)
                base_urls = [image_map[n] for n in neighbor_names if n in image_map]

                imgs = []
                for url in base_urls:
                    original, _ = get_image_cached(url) # ğŸ‘ˆ get_image_cachedê°€ RGBë¡œ ë³€í™˜
                    if original:
                        imgs.append(original.copy())

                if not imgs:
                    print(f"âš ï¸ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ (key: {key})")
                    continue

                merged = merge_images_vertically(imgs)
                display_w, display_h = crop["display_w"], crop["display_h"]
                scale_x, scale_y = merged.width / display_w, merged.height / display_h
                x, y, w, h = (
                    int(crop["x"] * scale_x),
                    int(crop["y"] * scale_y),
                    int(crop["w"] * scale_x),
                    int(crop["h"] * scale_y),
                )
                cropped = merged.crop((x, y, x + w, y + h))

                cropped.thumbnail((1280, 1280))
                cropped.save(img_path, quality=85) # ğŸ‘ˆ JPEGë¡œ ì €ì¥

            except Exception as e:
                print(f"âŒ Failed to generate preview for {key}: {e}")
                traceback.print_exc()
                continue

        try:
            with open(img_path, "rb") as f:
                encoded = b64encode(f.read()).decode("utf-8")
            previews[key] = f"data:image/jpeg;base64,{encoded}"
        except Exception as e:
            print(f"âŒ Preview encode failed for {key}: {e}")

    print(f"âœ… Preview returned {len(previews)} items")
    return jsonify({"ok": True, "previews": previews})


# --------------------------------------------------
# 6ï¸âƒ£ Save (ì›ë³¸ í•´ìƒë„ ê¸°ì¤€)
# --------------------------------------------------
@app.route("/save_crops", methods=["POST"])
def save_crops():
    payload = request.get_json()
    job_id = payload.get("job_id")
    data = payload.get("guides", {})

    if not job_id:
        return jsonify({"ok": False, "message": "Job IDê°€ ì—†ìŠµë‹ˆë‹¤."}), 400

    print(f"ğŸ¯ Crop Data Received: Job ID {job_id}, {len(data)} items")

    job_data = load_job(job_id)
    if not job_data:
        return jsonify({"ok": False, "message": f"Job not found (ID: {job_id})."}), 400

    if "scenes" not in job_data:
        print(f"âŒ Job íŒŒì¼ì— 'scenes' í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. (ID: {job_id})")
        return jsonify({"ok": False, "message": "Job data is incomplete (missing 'scenes')."}), 400

    job_output_dir = os.path.join(OUTPUT_DIR, job_id)
    os.makedirs(job_output_dir, exist_ok=True)

    image_map = {img["name"]: img["url"] for img in job_data.get("all_images", [])}

    parent_gdrive_folder_id = job_data.get("gdrive_folder_id")
    service = None
    target_upload_folder_id = None # ìµœì¢… ì—…ë¡œë“œë  GDrive í´ë” ID

    if parent_gdrive_folder_id:
        print(f"ğŸš€ Google Drive ì—…ë¡œë“œë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤. (ìƒìœ„ í´ë” ID: {parent_gdrive_folder_id})")
        service = get_gdrive_service()

        if service:
            folder_name_to_create = job_id
            new_folder_id = create_gdrive_folder(service, folder_name_to_create, parent_gdrive_folder_id)

            if new_folder_id:
                target_upload_folder_id = new_folder_id
            else:
                print(f"âš ï¸ {job_id} ì„œë¸Œí´ë” ìƒì„± ì‹¤íŒ¨. ìƒìœ„ í´ë”({parent_gdrive_folder_id})ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.")
                target_upload_folder_id = parent_gdrive_folder_id
        else:
            print("âš ï¸ Google Drive ì„œë¹„ìŠ¤ ì¸ì¦ ì‹¤íŒ¨, ë¡œì»¬ì—ë§Œ ì €ì¥í•©ë‹ˆë‹¤.")
    else:
        print("â„¹ï¸ Google Drive í´ë” IDê°€ jobì— ì—†ìŠµë‹ˆë‹¤. ë¡œì»¬ì—ë§Œ ì €ì¥í•©ë‹ˆë‹¤.")

    upload_count = 0
    processed_files = [] # â­ï¸ [ê³ ë„í™”] ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸

    for key, crop in data.items():
        if crop["w"] <= 1 or crop["h"] <= 1:
            print(f"â„¹ï¸ í¬ê¸°ê°€ 1ë³´ë‹¤ ì‘ì€ crop ë°ì´í„°ëŠ” ê±´ë„ˆëœë‹ˆë‹¤: {key}")
            continue
        try:
            scene_num, cut_type = key.split("_")
        except ValueError:
            print(f"âš ï¸ ì˜ëª»ëœ key í˜•ì‹ì…ë‹ˆë‹¤: {key}")
            continue

        scene_info = next(
            (s for s in job_data["scenes"] if str(s["scene_number"]) == scene_num),
            None,
        )
        if not scene_info:
            print(f"âš ï¸ ì”¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: ì”¬ {scene_num}")
            continue

        try:
            base_name = (
                scene_info["primary_cut"]
                if "primary" in cut_type
                else scene_info["alternative_cuts"][int(cut_type[-1]) - 1]
            )
        except Exception as e:
            print(f"âŒ ì”¬ ì •ë³´ì—ì„œ base_nameì„ ì°¾ëŠ” ì¤‘ ì˜¤ë¥˜ (key: {key}): {e}")
            continue

        # ğŸ‘ˆ [ìˆ˜ì •] ì´ í•¨ìˆ˜ê°€ ì´ì œ PNGë„ ì˜ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        neighbor_names = get_adjacent_names(base_name)
        base_urls = [image_map[n] for n in neighbor_names if n in image_map]
        imgs = []

        for url in base_urls:
            original, _ = get_image_cached(url) # ğŸ‘ˆ get_image_cachedê°€ RGBë¡œ ë³€í™˜
            if original:
                imgs.append(original.copy())
        if not imgs:
            print(f"âš ï¸ ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ (key: {key}, base_name: {base_name})")
            continue

        merged = merge_images_vertically(imgs)

        display_w, display_h = crop["display_w"], crop["display_h"]
        scale_x, scale_y = merged.width / display_w, merged.height / display_h
        x, y, w, h = (
            int(crop["x"] * scale_x),
            int(crop["y"] * scale_y),
            int(crop["w"] * scale_x),
            int(crop["h"] * scale_y),
        )

        try:
            cropped = merged.crop((x, y, x + w, y + h))
            # ğŸ‘ˆ [ì°¸ê³ ] ìµœì¢… ì €ì¥ íŒŒì¼ì€ í•­ìƒ .jpg ì…ë‹ˆë‹¤.
            out_path = os.path.join(job_output_dir, f"{key}.jpg")

            # 1. ë¡œì»¬ì— ì €ì¥
            cropped.save(out_path, quality=90) # ğŸ‘ˆ JPEGë¡œ ì €ì¥
            print(f"âœ… Saved merged crop locally: {out_path}")

            # 2. Google Driveì— ì—…ë¡œë“œ
            if service and target_upload_folder_id:
                try:
                    file_name = f"{key}.jpg" # ğŸ‘ˆ GDriveì—ë„ .jpgë¡œ ì—…ë¡œë“œ
                    img_buffer = BytesIO()
                    cropped.save(img_buffer, format="JPEG", quality=90)
                    img_buffer.seek(0)

                    file_metadata = {
                        'name': file_name,
                        'parents': [target_upload_folder_id]
                    }

                    media = MediaIoBaseUpload(
                        img_buffer,
                        mimetype='image/jpeg', # ğŸ‘ˆ MIME íƒ€ì…ë„ JPEG
                        resumable=True
                    )

                    file = service.files().create(
                        body=file_metadata,
                        media_body=media,
                        fields='id, webViewLink', # â­ï¸ [ê³ ë„í™”] ì›¹ ë§í¬ë„ í•¨ê»˜ ë°›ì•„ì˜¤ê¸°
                        supportsAllDrives=True
                    ).execute()

                    print(f"ğŸš€ Google Drive ì—…ë¡œë“œ ì„±ê³µ: {file_name} (File ID: {file.get('id')})")
                    upload_count += 1
                    # â­ï¸ [ê³ ë„í™”] ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´ ì¶”ê°€ (n8nìœ¼ë¡œ ë³´ë‚¼ ë°ì´í„°)
                    processed_files.append({
                        "key": key,
                        "file_name": file_name,
                        "gdrive_id": file.get('id'),
                        "gdrive_link": file.get('webViewLink')
                    })

                except Exception as e:
                    print(f"âŒ Google Drive ì—…ë¡œë“œ ì‹¤íŒ¨ ({key}): {e}")
                    # traceback.print_exc() # ìƒì„¸ ë¡œê·¸ í•„ìš” ì‹œ ì£¼ì„ í•´ì œ

        except Exception as e:
            print(f"âŒ í¬ë¡­ ì´ë¯¸ì§€ ì €ì¥/ì—…ë¡œë“œ ì‹¤íŒ¨ (key: {key}): {e}")
            traceback.print_exc()

    # --- ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ë£¨í”„ ì¢…ë£Œ ---

    # â­ï¸ [ê³ ë„í™”] n8nìœ¼ë¡œ ì™„ë£Œ ì•Œë¦¼ ë³´ë‚´ê¸°
    n8n_webhook_url = job_data.get("n8n_webhook_url")
    if n8n_webhook_url:
        print(f"ğŸ”” n8nìœ¼ë¡œ ì‘ì—… ì™„ë£Œ ì•Œë¦¼ì„ ë³´ëƒ…ë‹ˆë‹¤... (URL: {n8n_webhook_url})")
        try:
            # n8nìœ¼ë¡œ ë³´ë‚¼ ë°ì´í„° êµ¬ì„±
            webhook_payload = {
                "job_id": job_id,
                "status": "completed",
                "total_guides": len(data),
                "uploaded_files_count": upload_count,
                "gdrive_target_folder_id": target_upload_folder_id,
                "processed_files": processed_files # ê°œë³„ íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
            }
            # POST ìš”ì²­ ë³´ë‚´ê¸° (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
            response = requests.post(n8n_webhook_url, json=webhook_payload, timeout=10)
            response.raise_for_status() # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
            print(f"âœ… n8n ì•Œë¦¼ ì „ì†¡ ì„±ê³µ! (ìƒíƒœ ì½”ë“œ: {response.status_code})")
        except requests.exceptions.RequestException as e:
            print(f"âŒ n8n ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
        except Exception as e:
            print(f"âŒ n8n ì•Œë¦¼ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            traceback.print_exc()
    else:
        print("â„¹ï¸ n8n Webhook URLì´ ì—†ì–´ ì•Œë¦¼ì„ ë³´ë‚´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # GUIì— ë³´ë‚¼ ìµœì¢… ì‘ë‹µ ë©”ì‹œì§€
    message = f"Merged crops saved locally! (Total {len(data)} items processed)"
    if service and target_upload_folder_id:
        message += f" | Google Drive uploads: {upload_count} successful."
        if target_upload_folder_id != parent_gdrive_folder_id:
             message += f" (Folder: {job_id})"

    return jsonify({"ok": True, "message": message})


# --------------------------------------------------
# 7ï¸âƒ£ ì‹¤í–‰
# --------------------------------------------------
if __name__ == "__main__":
    print("ğŸ“œ Registered routes:")
    for rule in app.url_map.iter_rules():
        print(f" - {rule.endpoint} ({rule.methods}) -> {rule}")
    # ğŸ‘ˆ [ì°¸ê³ ] debug=Falseë¡œ ë³€ê²½í•˜ì…¨ë„¤ìš”! (ì´ì „ ì½”ë“œì™€ ë‹¤ë¦„)
    app.run(host="0.0.0.0", port=5000, debug=True, threaded=True)
